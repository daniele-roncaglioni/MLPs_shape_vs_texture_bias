{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:/Users/matth/Documents/ETHZ/01_DS/02_HS23/02_DeepLearning/03_Project/00_Testbed_DL/scaling_mlps_mirror')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import time\n",
    "\n",
    "# Attibution maps\n",
    "from captum.attr import IntegratedGradients, Saliency, InputXGradient, GuidedBackprop, NoiseTunnel, LRP, DeepLift\n",
    "\n",
    "# MLP model\n",
    "from data_utils.data_stats import *\n",
    "from models.networks import *\n",
    "from utils.download import *\n",
    "\n",
    "# CNN model\n",
    "from torchvision.models import resnet50, ResNet50_Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load MLP\n",
    "dataset = 'imagenet'                 # One of cifar10, cifar100, stl10, imagenet or imagenet21\n",
    "architecture = 'B_12-Wi_1024'        #'B_6-Wi_512'         #'B_12-Wi_1024'  'B-12_Wi-1024_res_64_imagenet_epochs_50'   \n",
    "resolution = 64                      # Resolution of fine-tuned model (64 for all models we provide)\n",
    "num_classes = CLASS_DICT[dataset]\n",
    "checkpoint = 'in21k_imagenet'        # This means you want the network pre-trained on ImageNet21k and finetuned on CIFAR10\n",
    "model_mlp = get_model(architecture=architecture, resolution=resolution, num_classes=num_classes,\n",
    "                  checkpoint=checkpoint, load_device='cpu', dropout=False)\n",
    "model_mlp.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load CNN\n",
    "model_cnn = resnet50(weights=ResNet50_Weights.IMAGENET1K_V1)\n",
    "for param in model_cnn.parameters():\n",
    "    param.requires_grad = False\n",
    "model_cnn.eval()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define plot generating functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_attribution_map(model, model_type, input_batch, target_y, algorithm, smooth_bool=False, ax=plt):  \n",
    "\n",
    "    # Compute attributions\n",
    "    if smooth_bool==False:\n",
    "        attr_algo = algorithm(model)\n",
    "        attributions = attr_algo.attribute(inputs=input_batch, target=target_y)\n",
    "    elif smooth_bool==True:\n",
    "        attr_algo = algorithm(model)\n",
    "        nt = NoiseTunnel(attr_algo)\n",
    "        stdev = float(np.std(input_batch.detach().numpy()))/3\n",
    "        attributions = nt.attribute(input_batch, target=target_y, nt_type='smoothgrad', stdevs=stdev, nt_samples=50) # smoothgrad, 'smoothgrad_sq', 'vargrad'\n",
    "\n",
    "    # Remove batch and channel dimensions\n",
    "    if model_type == \"CNN\": \n",
    "        attribution_array = attributions.squeeze(0).permute(1, 2, 0).detach().numpy()\n",
    "        attribution_array_abs = np.absolute(attribution_array)\n",
    "        attribution_array_abs_max = np.max(attribution_array_abs, axis=2)\n",
    "    elif model_type == \"MLP\":\n",
    "        attribution_array = attributions.detach().numpy().reshape([1, 3, resolution, resolution]).squeeze(0)\n",
    "        attribution_array_abs = np.absolute(attribution_array)\n",
    "        attribution_array_abs_max = np.max(attribution_array_abs, axis=0)\n",
    "\n",
    "    # Display the original size image\n",
    "    ax.imshow(attribution_array_abs_max, cmap='gray', vmin=np.percentile(attribution_array_abs_max, 1), vmax = np.percentile(attribution_array_abs_max, 99))\n",
    "    \n",
    "    # Hide X and Y axes label marks\n",
    "    ax.xaxis.set_tick_params(labelbottom=False)\n",
    "    ax.yaxis.set_tick_params(labelleft=False)\n",
    "\n",
    "    # Hide X and Y axes tick marks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Remove frame\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_original_image(original_image, ax=plt):\n",
    "\n",
    "    # Plot image\n",
    "    ax.imshow(input_image)\n",
    "    \n",
    "    # Hide X and Y axes label marks\n",
    "    ax.xaxis.set_tick_params(labelbottom=False)\n",
    "    ax.yaxis.set_tick_params(labelleft=False)\n",
    "\n",
    "    # Hide X and Y axes tick marks\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "\n",
    "    # Remove frame\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_visible(False)\n",
    "    ax.spines['left'].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify path\n",
    "#path = \"C:\\\\Users\\\\matth\\Documents\\\\ETHZ\\\\01_DS\\\\02_HS23\\\\02_DeepLearning\\\\03_Project\\\\00_Working\\\\model-vs-human\\\\datasets\\\\colour\\\\dnn\\\\session-1\\\\\"\n",
    "path = './source_images/'\n",
    "\n",
    "\n",
    "## Selection short\n",
    "filename_arr_short = np.array([\n",
    "    \"0088_cl_dnn_cr_bear_40_n02133161_2202.png\", # 295\tAmerican black bear\n",
    "    \"0110_cl_dnn_cr_elephant_40_n02504458_1600.png\", # 386 african elephant\n",
    "    \"0509_cl_dnn_cr_chair_40_n04099969_4543.png\", # 765 rocking chair\n",
    "    \"0079_cl_dnn_cr_clock_40_n04548280_77.png\", # 892 wall clock\n",
    "    ])\n",
    "target_y_arr_short = np.array([295, 385, 765, 892]) \n",
    "\n",
    "\n",
    "## Selection long\n",
    "filename_arr_long = np.array([\n",
    "    \"0088_cl_dnn_cr_bear_40_n02133161_2202.png\", # 295\tAmerican black bear\n",
    "    \"0528_cl_dnn_cr_bear_40_n02133161_1362.png\", # 297 sloth bear\n",
    "    \"0110_cl_dnn_cr_elephant_40_n02504458_1600.png\", # 386 african elephant\n",
    "    \"0133_cl_dnn_cr_elephant_40_n02504013_4892.png\", # 385 indian elephant\n",
    "    \"0447_cl_dnn_cr_chair_40_n03376595_1302.png\", # 559 folding chair\n",
    "    \"0509_cl_dnn_cr_chair_40_n04099969_4543.png\", # 765 rocking chair\n",
    "    \"0279_cl_dnn_cr_clock_40_n04548280_24041.png\", # 892 wall clock\n",
    "    \"0079_cl_dnn_cr_clock_40_n04548280_77.png\", # 892 wall clock\n",
    "    ])\n",
    "target_y_arr_selection = np.array([295, 297, 386, 385, 559, 765, 892, 892]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Select selection(s) to generate attribution maps for\n",
    "filename_arr_arr = np.array([filename_arr_short])\n",
    "target_y_arr_arr = np.array([target_y_arr_short])\n",
    "path = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate attribution maps for specified selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (filename_arr, target_y_arr) in zip(filename_arr_arr, target_y_arr_arr):\n",
    "\n",
    "    # Initialize plot\n",
    "    imax = filename_arr.shape[0]\n",
    "    fig, axs = plt.subplots(imax,5,figsize=(2*5,2*imax))\n",
    "    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n",
    "\n",
    "    # Create subplots\n",
    "    for iter, (filename, target_y) in enumerate(zip(filename_arr[:imax], target_y_arr[:imax])):\n",
    "            \n",
    "        target_y = int(target_y)\n",
    "        print(\"Iterator:\", iter)\n",
    "        print(\"Filename:\", filename)\n",
    "        print(\"Target Y:\", target_y)\n",
    "        \n",
    "        ##--------------------------------------------\n",
    "\n",
    "        # Load image\n",
    "        filepath = path + filename\n",
    "        input_image = Image.open(filepath)\n",
    "\n",
    "        ##--------------------------------------------\n",
    "\n",
    "        ## Prepare input for MLP\n",
    "        preprocess_mlp = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.ConvertImageDtype(torch.float32),\n",
    "            transforms.Resize(size=(resolution, resolution), antialias=True),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        input_batch_mlp = preprocess_mlp(input_image).unsqueeze(0)\n",
    "        input_batch_mlp_reshaped = torch.reshape(input_batch_mlp, (input_batch_mlp.shape[0], -1))\n",
    "        input_batch_mlp_reshaped = Variable(input_batch_mlp_reshaped, requires_grad=True)\n",
    "\n",
    "        ##--------------------------------------------\n",
    "        \n",
    "        ## Prepare input for CNN\n",
    "        preprocess_cnn = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "        input_batch_cnn = preprocess_cnn(input_image).unsqueeze(0)\n",
    "        \n",
    "        ##--------------------------------------------\n",
    "\n",
    "        # Create saliency maps\n",
    "        plot_original_image(original_image=input_image, ax=axs[iter,0])\n",
    "        create_attribution_map(model=model_mlp, model_type=\"MLP\", input_batch=input_batch_mlp_reshaped, target_y=target_y, algorithm=Saliency, smooth_bool=True, ax=axs[iter,1])\n",
    "        create_attribution_map(model=model_mlp, model_type=\"MLP\", input_batch=input_batch_mlp_reshaped, target_y=target_y, algorithm=InputXGradient, smooth_bool=True, ax=axs[iter,2])\n",
    "        create_attribution_map(model=model_cnn, model_type=\"CNN\", input_batch=input_batch_cnn, target_y=target_y, algorithm=Saliency, smooth_bool=True, ax=axs[iter,3])\n",
    "        create_attribution_map(model=model_cnn, model_type=\"CNN\", input_batch=input_batch_cnn, target_y=target_y, algorithm=InputXGradient, smooth_bool=True, ax=axs[iter,4])\n",
    "\n",
    "        if iter==0:\n",
    "            # Set titles\n",
    "            fontsize = 10\n",
    "            axs[iter,1].set_title('Gradient: \\n B-12/Wi-1024+DA', fontsize=fontsize)\n",
    "            axs[iter,2].set_title('Input * Gradient: \\n B-12/Wi-1024+DA', fontsize=fontsize)\n",
    "            axs[iter,3].set_title('Gradient: \\n ResNet-50', fontsize=fontsize)\n",
    "            axs[iter,4].set_title('Input * Gradient: \\n ResNet-50', fontsize=fontsize)\n",
    "\n",
    "    # Specify the folder path where you want to save the plot\n",
    "    output_folder = './output_images'\n",
    "    # Ensure the output folder exists or create it if not\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "    # Save the plot to the specified folder\n",
    "    timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "    output_filename = timestr + '.png'\n",
    "    output_filepath = os.path.join(output_folder, output_filename)\n",
    "    plt.savefig(output_filepath)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
